{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Ukfr07CaAE"
   },
   "source": [
    "# Uncertainty Estimation Practical Part I\n",
    "There are many ways to estimate predictive uncertainty in deep learning. However, in this practical, we focus on simple yet effective methods. One such method that has achieved impressive performance is the Deep Ensembles method[[1]]. In this practical, we first study this method in Part I, then a recent extension in Part II.\n",
    "\n",
    "[1]:https://arxiv.org/pdf/1612.01474.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVhXsBnNCaAH"
   },
   "source": [
    "## Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles -- Lakshminarayanan B, Pritzel A, Blundell C. [[1]]\n",
    "\n",
    "### Abstract:\n",
    "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.\n",
    "\n",
    "### Notations\n",
    "Assume we have i.i.d. training data points $(\\boldsymbol{x}, y)$, where $\\boldsymbol{x} \\in \\mathbb{R}^D$ is the D-dimensional input features. And for a classification problem with $K$ classes, $y \\in \\{1,\\ldots,K\\}$; for a regression problem, $y \\in \\mathbb{R}$. We use a NN with parameters $\\theta$ to model the probablistic predictive distribution $p_{\\theta}(y|\\boldsymbol{x})$ over the labels.\n",
    "\n",
    "### Proper scoring rules\n",
    "Scoring rules measure the quality of predictive uncertainty [[2]]. Suppose we have $(y,\\boldsymbol{x})$ following the true distribution $q(y,\\boldsymbol{x})$, and we want to evaluate the quality of the predictive distribution, $p_{\\theta}(y|\\boldsymbol{x})$. The scoring rule, a function, $S(p_{\\theta},(y,\\boldsymbol{x}))$, assigns a numerical score to the predictive distribution $p_{\\theta}(y|\\boldsymbol{x})$. Here we consider scoring rules where a high score means better quality. The expected scoring rule is then $S_{\\mathbb{E}}(p_{\\theta},q)=\\int q(y,\\boldsymbol{x}) S(p_{\\theta},(y,\\boldsymbol{x}))dyd\\boldsymbol{x}$. A *proper scoring rule* is, for all $p_{\\theta}$ and $q$, $S_{\\mathbb{E}}(p_{\\theta},q) \\leq S_{\\mathbb{E}}(q,q)$ with equality if and only if $p_{\\theta}(y|\\boldsymbol{x})=q(y|\\boldsymbol{x})$. Since a larger value $S_{\\mathbb{E}}(p_{\\theta},q)$ means a better quality of the predictive uncertainty, we could train NNs by minimizing the loss $\\mathcal{L}(\\theta)=-S_{\\mathbb{E}}(p_{\\theta},q)$.\n",
    "\n",
    "\n",
    "Log-likelihood, $\\log p_{\\theta}(y|\\boldsymbol{x})$, turns out to be a proper scoring rule due to [Gibbs inequality]:\n",
    "\n",
    "\\begin{equation}\n",
    "S_{\\mathbb{E}}(p_{\\theta},q)=\\mathbb{E}_{q(\\boldsymbol{x})} q(y|\\boldsymbol{x}) \\log p_{\\theta}(y|\\boldsymbol{x}) \\leq \\mathbb{E}_{q(\\boldsymbol{x})} q(y|\\boldsymbol{x}) \\log q(y|\\boldsymbol{x}). \n",
    "\\end{equation}\n",
    "\n",
    "Thus minimizing the negative log-likelihood (NLL), which is equivalent to the softmax cross entropy loss in classification, is a proper scoring rule. Interestingly minimizing the squared error between the predictive probability of a label and one-hot encoding of the correct label, is also a proper scoring rule known as the Brier score. In regression, if we assume $p_{\\theta}$ to be Gaussian $\\mathcal{N}(\\mu_{\\theta}(\\boldsymbol{x}), \\sigma_{\\theta}^2(\\boldsymbol{x}))$: \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "-\\log p_{\\theta}(y|\\boldsymbol{x}) &= -\\log \\bigg(\\frac{1}{\\sqrt{2\\pi\\sigma_{\\theta}^2(\\boldsymbol{x})}} \\exp\\big(-\\frac{(y-\\mu_{\\theta}(\\boldsymbol{x}))^2}{2\\sigma_{\\theta}^2(\\boldsymbol{x})}\\big)  \\bigg) \\\\\n",
    "&=\\log \\big( \\sqrt{2\\pi\\sigma_{\\theta}^2(\\boldsymbol{x})} \\big) + \\frac{(y-\\mu_{\\theta}(\\boldsymbol{x}))^2}{2\\sigma_{\\theta}^2(\\boldsymbol{x})}\\\\\n",
    "&=\\frac{\\log 2\\pi\\sigma_{\\theta}^2(\\boldsymbol{x})}{2} + \\frac{(y-\\mu_{\\theta}(\\boldsymbol{x}))^2}{2\\sigma_{\\theta}^2(\\boldsymbol{x})}\\\\\n",
    "&=\\frac{\\log \\sigma_{\\theta}^2(\\boldsymbol{x})}{2}+\\frac{(y-\\mu_{\\theta}(\\boldsymbol{x}))^2}{2\\sigma_{\\theta}^2(\\boldsymbol{x})}+\\text{constant}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Of course we can model the $p_{\\theta}$ to be other distributions, e.g. [Laplacian distribution], which corresponds to L1 loss, MAE, rather than Gaussian distribution, which corresponds to L2 loss, MSE. If we assume $p_{\\theta}$ to be Laplacian $\\mathcal{L}(\\mu_{\\theta}(\\boldsymbol{x}), \\sigma_{\\theta}(\\boldsymbol{x}))$, with mean $\\mu_{\\theta}(\\boldsymbol{x})$ and variance $\\sigma^2_{\\theta}(\\boldsymbol{x})$:\n",
    "\n",
    "====Derive the NLL loss for Laplacian assumption====\n",
    "\\begin{align}\n",
    "-\\log p_{\\theta}(y|\\boldsymbol{x}) &=\n",
    "\\end{align}\n",
    "### Adversarial training\n",
    "Adversarial examples (see [[3]] and [[4]]) are those which are very close to the original training examples but are misclassified by the NN. In [[4]], the authors proposed the *fast gradient sign method* as a fast solution to generate adversarial examples. Given $(\\boldsymbol{x},y)$ and the loss $\\ell(\\theta,\\boldsymbol{x},y)$, we can use the fast gradient sign method to generate an adversarial example as $\\boldsymbol{x}'=\\boldsymbol{x}+\\epsilon \\text{sign}\\big(\\nabla_{\\boldsymbol{x}}\\ell(\\theta,\\boldsymbol{x},y)\\big)$, where $\\epsilon$ is a small value to bound the perturbation. The procedure to use this adversarial example as an additional training sample to augument the training set, referred to as *adversarial training* was found to improve the classifier's robustness [[4]].  \n",
    "\n",
    "There are more than one way to generate adversarial examples, for example, *virtual adversarial training* (VAT)[[5]], which does not require to know the true target $y$, could be used in *semi-supervised learning* (SSL).\n",
    "\n",
    "### Ensembles\n",
    "In general there are two classes of ensembles: *randomization*-based approaches and *boosting*-based approaches. Here we only focus on randomization-based approaches since it is more suitable for parallel computation. In practice, random initialization of the NN parameters $\\theta$ and with random shuffling of the data points during training were found to be sufficient to obtain good performance. During inference, the ensemble is treated as a uniformly-weighted mixture model and the predictions are combined as $p(y|\\boldsymbol{x})=M^{-1}\\sum_{m=1}^{M}p_{\\theta_m}(y|\\boldsymbol{x},\\theta_m)$. For classification, this corresponds to averaging the predicted probabilities. For regression, if we assume Gaussian, then the prediction is a mixture of Gaussian distribution and we approximate the ensemble prediction as a Gaussian whose mean and variance are respectively the mean and variance of the mixture:\n",
    "\\begin{align}\n",
    "\\mu_{\\ast}(\\boldsymbol{x})&=M^{-1}\\sum_{m=1}^{M}\\mu_{\\theta_m}(\\boldsymbol{x})\\\\\n",
    "\\sigma^2_{\\ast}(\\boldsymbol{x})&=M^{-1}\\sum_{m=1}^{M}(\\sigma^2_{\\theta_m}(\\boldsymbol{x}) + \\mu^2_{\\theta_m}(\\boldsymbol{x})) - \\mu^2_{\\ast}(\\boldsymbol{x}).\n",
    "\\end{align}\n",
    "\n",
    "And if we assume Laplacian, then the mean and variance of the mixture is:\n",
    "\\begin{align}\n",
    "\\mu_{\\ast}(\\boldsymbol{x})&=M^{-1}\\sum_{m=1}^{M}\\mu_{\\theta_m}(\\boldsymbol{x})\\\\\n",
    "\\sigma^2_{\\ast}(\\boldsymbol{x})&=M^{-1}\\sum_{m=1}^{M}(2\\sigma^2_{\\theta_m}(\\boldsymbol{x}) + \\mu^2_{\\theta_m}(\\boldsymbol{x})) - \\mu^2_{\\ast}(\\boldsymbol{x}).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "[1]:https://arxiv.org/pdf/1612.01474.pdf\n",
    "[2]:https://viterbi-web.usc.edu/~shaddin/cs699fa17/docs/GR07.pdf \n",
    "[3]:https://arxiv.org/pdf/1312.6199.pdf\n",
    "[4]:https://arxiv.org/pdf/1412.6572.pdf\n",
    "[5]:https://arxiv.org/abs/1704.03976\n",
    "[Gibbs inequality]:https://en.wikipedia.org/wiki/Gibbs%27_inequality\n",
    "[Laplacian distribution]:https://en.wikipedia.org/wiki/Laplace_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJfm0yoxCaAL"
   },
   "source": [
    "## Regression on toy dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcGi2MKhCaAM"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt \n",
    "from jax.scipy.special import logsumexp\n",
    "import numpy as np\n",
    "from jax.experimental import optimizers\n",
    "import time \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQYtKWe4CaAN"
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key, scale):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, scale) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "def predict(params, inputs):\n",
    "    # per-example predictions\n",
    "    activations = inputs\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = jax.nn.relu(outputs)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    \n",
    "    \n",
    "    mean, variance = jnp.split(logits, 2)\n",
    "    \n",
    "\n",
    "    variance = jax.nn.softplus(variance) + 1e-6\n",
    "    return mean, variance # for NLL loss\n",
    "    # return logits # for MSE loss\n",
    "\n",
    "def loss(params, inputs, targets):\n",
    "    preds = batched_predict(params, inputs)\n",
    "    return jnp.mean(jnp.square(preds - targets))\n",
    "\n",
    "def loss_l1(params, inputs, targets):\n",
    "    preds = batched_predict(params, inputs)\n",
    "    return jnp.mean(jnp.absolute(preds - targets))\n",
    "\n",
    "def NLLloss_gaussian(params, inputs, targets):\n",
    "    \"\"\" \n",
    "    Negative log-likelihood loss function. \n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The ``inputs`` argument and ``targets`` of this function are both of shape (N,1)=(#examples, 1).\n",
    "    1. Feed forward the data to NN to obtain mean and variance\n",
    "    2. Calculate the negative log-likelihood using the outputs of the NN, based on Gaussian assumption (the loss should be a scalar)\n",
    "    \"\"\"\n",
    "    return nll_loss\n",
    "\n",
    "def NLLloss_laplacian(params, inputs, targets):\n",
    "    \"\"\" \n",
    "    Negative log-likelihood loss function. \n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The ``inputs`` argument and ``targets`` of this function are both of shape (N,1)=(#examples, 1).\n",
    "    1. Feed forward the data to NN to obtain mean and variance\n",
    "    2. Calculate the negative log-likelihood using the outputs of the NN, based on Laplacian assumption (the loss should be a scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    return nll_loss\n",
    "\n",
    "\n",
    "def NLLlossAT_gaussian(params, inputs, targets, eps=0.08):\n",
    "    \"\"\" \n",
    "    Negative log-likelihood loss function with adversarial training \n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The ``inputs`` argument and ``targets`` of this function are both of shape (N,1)=(#examples, 1).\n",
    "    1. Feed forward the data to NN to obtain mean and variance\n",
    "    2. Calculate the negative log-likelihood using the outputs of the NN, based on Gaussian assumption (the loss should be a scalar)\n",
    "    3. Calculate the gradient\n",
    "    4. Generate adversarial examples\n",
    "    5. Calculate the NLL loss based on the adversarial examples and add the two losses together\n",
    "    \"\"\"\n",
    "    \n",
    "    return (nll + nll_ad)/2\n",
    "\n",
    "def NLLlossAT_laplacian(params, inputs, targets, eps=0.08):\n",
    "    \"\"\" \n",
    "    Negative log-likelihood loss function with adversarial training \n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The ``inputs`` argument and ``targets`` of this function are both of shape (N,1)=(#examples, 1).\n",
    "    1. Feed forward the data to NN to obtain mean and variance\n",
    "    2. Calculate the negative log-likelihood using the outputs of the NN, based on Laplacian assumption (the loss should be a scalar)\n",
    "    3. Calculate the gradient\n",
    "    4. Generate adversarial examples\n",
    "    5. Calculate the NLL loss based on the adversarial examples and add the two losses together\n",
    "    \"\"\"\n",
    "\n",
    "    return (nll + nll_ad)/2\n",
    "\n",
    "\n",
    "@jit\n",
    "def update(params, x, y, opt_state):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads = value_and_grad(loss)(params, x, y)\n",
    "    #value, grads = value_and_grad(NLLloss_gaussian)(params, x, y)\n",
    "    #value, grads = value_and_grad(NLLlossAT_gaussian)(params, x, y)\n",
    "\n",
    "\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value\n",
    "\n",
    "def data_set(points=20, xrange=(-4, 4), std=3.):\n",
    "    xx = jnp.array([[np.random.uniform(*xrange)] for i in range(points)])\n",
    "    yy = jnp.array([[x**3 + np.random.normal(0, std)] for x in xx])\n",
    "    return xx.reshape(-1,1), yy.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoG0Sk3xCaAO"
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "\n",
    "layer_sizes = [1, 10, 6, 30, 2] \n",
    "\n",
    "# param_scale = 0.02\n",
    "num_epochs = 600\n",
    "step_size = 8e-2\n",
    "\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "\n",
    "# to ensure reproducibility\n",
    "np.random.seed(173)\n",
    "xx, yy = data_set(points=40, xrange=(-4, 4), std=3.) # generate data set of 20 samples\n",
    "x = np.linspace(-6, 6, 100).reshape(-1, 1)\n",
    "y = x**3\n",
    "x = jnp.array(x)\n",
    "y = jnp.array(y)\n",
    "\n",
    "# number of ensembles \n",
    "M = 5\n",
    "params_all = [None]*M\n",
    "\n",
    "opt_state_all = [None]*M\n",
    "mean_all, var_all = [], []\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "\n",
    "for i in range(M):\n",
    "    # use random seed to randomly initialize the NN\n",
    "    SEED = 100+abs(int(np.random.normal(0,1)*10000))\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    param_scale = abs(np.random.normal(0,0.1))\n",
    "    print(\"----------training network \"+str(i)+\"------------param_scale \"+str(param_scale))\n",
    "    params_all[i] = init_network_params(layer_sizes, random.PRNGKey(i*10), param_scale)\n",
    "    \n",
    "    opt_state_all[i] = opt_init(params_all[i])\n",
    "\n",
    "    log_acc_train, log_train_loss = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        params_all[i] = get_params(opt_state_all[i])\n",
    "        params_all[i], opt_state_all[i], train_loss = update(params_all[i], xx, yy, opt_state_all[i])\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(\"initial loss: \",train_loss)\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        log_train_loss.append(train_loss)\n",
    "\n",
    "    print(\"final loss: \",train_loss)\n",
    "\n",
    "    mean, var = batched_predict(params_all[i], x)\n",
    "    \n",
    "\n",
    "    \n",
    "    mean_all.append(mean)\n",
    "    var_all.append(var)\n",
    "\n",
    "mean_ = np.mean(np.array([ mean for mean in mean_all]), axis=0)\n",
    "var_ = np.zeros_like(var_all[0])\n",
    "for i in range(M):\n",
    "    \"\"\" \n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    Calculate the variance for the mixture model, according to your assumption, Gaussian or Laplacian.\n",
    "    \"\"\"\n",
    "    var_+= \n",
    "    \n",
    "var_/=M\n",
    "std_ = np.sqrt(var_)\n",
    "\n",
    "for i in range(M):\n",
    "    plt.figure()\n",
    "    plt.plot(x, y, \"b-\", label=\"ground truth: $y=x^3$\")\n",
    "    plt.plot(xx,yy,\"or\", label=\"data points\")\n",
    "    plt.plot(x, mean_all[i], label=\"mean prediction\", color=\"grey\")\n",
    "    plt.fill_between(x.reshape(-1,), (mean_all[i]-np.sqrt(var_all[i])).reshape(-1,), (mean_all[i]+np.sqrt(var_all[i])).reshape(-1,),alpha=0.5)\n",
    "    plt.grid()\n",
    "    plt.title(\"MLP \" + str(i))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.ylim((-300,300))\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, \"b-\", label=\"ground truth: $y=x^3$\")\n",
    "plt.plot(xx,yy,\"or\", label=\"data points\")\n",
    "plt.plot(x, mean_, label=\"mean prediction\", color=\"grey\")\n",
    "plt.fill_between(x.reshape(-1,), (mean_-std_).reshape(-1,), (mean_+std_).reshape(-1,),alpha=0.5)\n",
    "plt.grid()\n",
    "plt.title(\"ensemble of \"+str(M)+\" MLPs\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.ylim((-300,300))\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgAjyBkjRCK8"
   },
   "source": [
    "**Discussion points:**\n",
    "* In terms of regression, compared to only use NLL loss with a single network, what is the effect of adding Adversarial Training? How about adding ensembles of M networks? \n",
    "* How is the number of networks, M, affecting the prediction and uncertainty estimation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGnUvj3OCaAQ"
   },
   "source": [
    "## Classification on [toy dataset] : \n",
    "[toy dataset]: https://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoRNH_GBCaAR"
   },
   "outputs": [],
   "source": [
    "%reset -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEA7sFXrCaAS"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt \n",
    "from jax.scipy.special import logsumexp\n",
    "import numpy as np\n",
    "from jax.experimental import optimizers\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sw_CPTO4fIh"
   },
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def generate_datasets(N,K,noise):\n",
    "    X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "\n",
    "    for j in range(K):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        r = np.linspace(0.0,1.,N) # radius\n",
    "        t = np.linspace(j*8,(j+1)*8,N) + np.random.randn(N)*noise * (r+1.0) # theta\n",
    "        print(j, np.amin(t), np.amax(t))\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    X = jnp.array(X)\n",
    "    y = jnp.array(y)\n",
    "    y_onehot = one_hot(y, K)\n",
    "    return X, y, y_onehot\n",
    "\n",
    "def shuffle_dataset(X, y, y_onehot, seed):\n",
    "    indices = jax.random.permutation(seed, jnp.arange(X.shape[0]))\n",
    "    X_shuffled = jnp.take(X, indices, axis=0)\n",
    "    y_oh_shuffled = jnp.take(y_onehot, indices, axis=0)\n",
    "    y_shuffled = y[indices]\n",
    "    return X_shuffled, y_shuffled, y_oh_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rv_yuUGlCN6a"
   },
   "outputs": [],
   "source": [
    "N = 1000 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, y_onehot = generate_datasets(N, K, noise=0.3)\n",
    "\n",
    "# lets visualize the data:\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Training Data\")\n",
    "print(\"Training X:\", X.shape)\n",
    "print(\"Training y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xVpimmT4ulT"
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-1):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "@jit\n",
    "def batched_predict(params, input):\n",
    "    def _predict(params, image):\n",
    "        # per-example predictions\n",
    "        activations = image\n",
    "        for w, b in params[:-1]:\n",
    "            outputs = jnp.dot(w, activations) + b\n",
    "            activations = jax.nn.relu(outputs)\n",
    "\n",
    "        final_w, final_b = params[-1]\n",
    "        logits = jnp.dot(final_w, activations) + final_b\n",
    "        probs = jax.nn.softmax(logits)\n",
    "        return logits, probs\n",
    "\n",
    "    return vmap(_predict, in_axes=(None, 0))(params, input)\n",
    "  \n",
    "def accuracy(params, inputs, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    logits, _ = batched_predict(params, inputs)\n",
    "    predicted_class = jnp.argmax(logits, axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, inputs, targets, wd):\n",
    "\n",
    "    logits, preds = batched_predict(params, inputs)\n",
    "    log_prob = logits - logsumexp(logits, axis=1)[:,None]\n",
    "    return -jnp.sum(log_prob * targets) / inputs.shape[0] + wd * jax.experimental.optimizers.l2_norm(params)\n",
    "\n",
    "\n",
    "def lossAT(params, inputs, targets, wd, eps=0.02):\n",
    "    \"\"\" \n",
    "    Negative log-likelihood loss function with adversarial training \n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The ``inputs`` argument and ``targets`` of this function are both of shape (N,1)=(#examples, 1).\n",
    "    1. Feed forward the data to NN to calculate the negative log-likelihood \n",
    "    2. Calculate the gradient\n",
    "    3. Generate adversarial examples\n",
    "    4. Calculate the NLL loss based on the adversarial examples and add the two losses together\n",
    "    \"\"\"\n",
    "    \n",
    "    return (loss_ori + loss_ad)/2\n",
    "    \n",
    "@jit\n",
    "def update(params, x, y, opt_state, wd):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads = value_and_grad(loss)(params, x, y, wd)\n",
    "    #value, grads = value_and_grad(lossAT)(params, x, y, wd)\n",
    "    \n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value\n",
    "\n",
    "def logging(params_all, X, y, K, M, log_acc_train, log_nll_train):\n",
    "\n",
    "    probs = jnp.mean( jnp.array([batched_predict(params_all[i], X)[1] for i in range(M)]), axis=0)\n",
    "\n",
    "    y_pred = jnp.argmax(probs,axis=1) \n",
    "    train_acc = jnp.mean(y_pred == y)   \n",
    "\n",
    "    y_conf = probs[np.arange(y.shape[0]),y]\n",
    "    train_nll = -jnp.mean(jnp.log(y_conf))\n",
    "\n",
    "    log_acc_train.append(train_acc)\n",
    "    log_nll_train.append(train_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBUW5dJ5CaAV"
   },
   "outputs": [],
   "source": [
    "layer_sizes = [2, 150, 150, 3]\n",
    "num_epochs = 300\n",
    "step_size = 0.1\n",
    "wd=1e-3\n",
    "\n",
    "M = 4\n",
    "K = layer_sizes[-1]\n",
    "params_all = [None]*M\n",
    "opt_state_all = [None]*M\n",
    "\n",
    "\n",
    "log_acc, log_nll = [], []\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "\n",
    "for i in range(M):\n",
    "    # use random seed to randomly initialize the NN\n",
    "    params_all[i] = init_network_params(layer_sizes, random.PRNGKey(i))\n",
    "    opt_state_all[i] = opt_init(params_all[i])\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(M):\n",
    "        key, seed = random.split(key)\n",
    "\n",
    "        X_shuffled, y_shuffled, y_oh_shuffled = shuffle_dataset(X, y, y_onehot, seed)\n",
    "\n",
    "        params_all[i] = get_params(opt_state_all[i])\n",
    "        params_all[i], opt_state_all[i], train_loss = update(params_all[i], X_shuffled, y_oh_shuffled, opt_state_all[i], wd)\n",
    "\n",
    "    logging(params_all, X, y, K, M, log_acc, log_nll) \n",
    "\n",
    "\n",
    "    print('\\r', f'[Epoch {epoch+1}]: Train Acc: {log_acc[-1]:.3f} | Train NLL: {log_nll[-1]:0.3f}', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlfkaoh8Igjq"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
    "fig.tight_layout()\n",
    "axes[0].plot(log_acc)\n",
    "axes[1].plot(log_nll)\n",
    "axes[0].title.set_text('Train Acc')\n",
    "axes[1].title.set_text('Train NLL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xjz3A5MeojM"
   },
   "source": [
    "### Measuring Uncertainty - Total, Knowledge and Data Uncertainty\n",
    "The Shannon entropy measures the information/uncertainty in a categorical distribution $\\boldsymbol{p}=[p_1,p_2,\\dots,p_K]$\n",
    "$$H(p) = -\\sum_{i=1}^K p_i\\log{p_i}$$ \n",
    "The entropy is high when the probability is spread out over the classes and low when most of the probability is in a single class.\n",
    "\n",
    "We can use entropy to measure the uncertainty in a prediction. Using the same notation as Malinin et al. [[1]], we call the average entropy of each prediction of the ensemble the **data uncertainty**. Let $\\boldsymbol{p}^{(1)}, \\dots,\\boldsymbol{p}^{(M)}$ be the M predictive distributions of the ensemble, then the data uncertainty is\n",
    "$$ \\frac{1}{M}\\sum_{i=1}^M H\\Big(\\boldsymbol{p}^{(i)}\\Big) $$\n",
    "\n",
    "The entropy of the mean prediction is called the **total uncertainty**, which is high if the data uncertainty is high or if the predictions are diverse. \n",
    "\n",
    "**Knowledge uncertainty** (think: uncertainty in parameters) is related to the diversity of the predictions, which we measure with the multi-distribution Jensen-Shannon divergence\n",
    "$$ H\\Big(\\frac{1}{M}\\sum_{i=1}^M \\boldsymbol{p}^{(i)}\\Big) - \\frac{1}{M}\\sum_{i=1}^M H\\Big(\\boldsymbol{p}^{(i)}\\Big) $$\n",
    "Indeed, the divergence measures the difference between total and data uncertainty. It is low when the predictions are similar and high when the predictions are different but confident.\n",
    "\n",
    "Below, we visualize the data and knowledge uncertainty for our deep ensemble.\n",
    "\n",
    "[1]:https://arxiv.org/abs/1905.00076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eZE208AKllmX"
   },
   "outputs": [],
   "source": [
    "def entropy(preds):\n",
    "    preds = np.clip(preds, 1e-7, 1.0)\n",
    "    return -np.sum( preds * np.log(preds), axis=1)\n",
    "\n",
    "def js_terms(distributions):\n",
    "    return entropy(np.mean(distributions, axis=0)), np.mean([entropy(p) for p in distributions], axis=0)\n",
    "\n",
    "def visualize_predictions(X, y, params_list, min=-2.0, max=2.0, res=200, num_nets=1):\n",
    "    xs = np.linspace(min, max, res)\n",
    "    ys = np.linspace(min, max, res)\n",
    "    N, M = len(xs), len(ys)\n",
    "    xy = np.asarray([(_x,_y) for _x in xs for _y in ys])\n",
    "    num_samples = xy.shape[0]\n",
    "\n",
    "    predictions = [batched_predict(params, xy)[1] for params in params_list]\n",
    "    predictions_ensemble = np.mean(predictions, axis=0)\n",
    "    total, data = js_terms(predictions)\n",
    "\n",
    "    Z, Z2, Z3 = np.zeros((N,M)), np.zeros((N,M)), np.zeros((N,M))\n",
    "    indices = np.unravel_index(np.arange(num_samples), (N,M))\n",
    "    Z[indices] = jnp.argmax(predictions_ensemble, axis=1)\n",
    "    Z2[indices] = total - data\n",
    "    Z3[indices] = data\n",
    "\n",
    "    fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "    axes = axes.flatten()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "    axes[1].contourf(xs, ys, Z.T, cmap=plt.cm.Spectral, levels=50)\n",
    "    axes[2].contourf(xs, ys, Z3.T, cmap='magma',levels=50)\n",
    "    axes[3].contourf(xs, ys, Z2.T, cmap='magma', levels=50)\n",
    "\n",
    "    axes[0].set_xlim([min, max]); axes[0].set_ylim([min, max]); \n",
    "\n",
    "    axes[0].title.set_text('Dataset')\n",
    "    axes[1].title.set_text('Mean')\n",
    "    axes[2].title.set_text('Data Uncertainty')\n",
    "    axes[3].title.set_text('Knowledge Uncertainty')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yr9DJMhqeh5H"
   },
   "outputs": [],
   "source": [
    "visualize_predictions(X, y, params_all, num_nets=M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0lQTx8Q3Plk"
   },
   "source": [
    "where dark to light increases in value. \n",
    "\n",
    "**Discussion points:**\n",
    "* Are there any regions where the network makes overconfident predictions? \n",
    "* Where is the ensemble most uncertain (high entropy)? Why?\n",
    "* Where are the members of the ensemble the most diverse? Why?\n",
    "* How does adversarial training affect the results? Why? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PartI.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "flax",
   "language": "python",
   "name": "flax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
